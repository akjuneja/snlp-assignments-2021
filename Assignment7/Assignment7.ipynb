{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feU_fawmgoPp"
      },
      "source": [
        "## Exercise 1: Count-Trees (5 points)\n",
        "\n",
        "Your task is to implement a count-tree with variable maximum history. It is a memory-efficient way to store n-gram counts which can also be used to create an intuitive back-off after the tree is pruned.\n",
        "\n",
        "The tree object should support the following four operations:\n",
        "- Increment a count of a specific n-gram (even if not present)\n",
        "- Retrieve counts given the history (variable length)\n",
        "- Retrieve the conditional probability of a word given the history. (Proportion count between branches)\n",
        "- Pruning all nodes with counts less or equal to $k$\n",
        "\n",
        "**1.1 (2 points)**\n",
        "\n",
        "Make sure your implementation is correct by passing the asserts in the first code cell.\n",
        "\n",
        "**1.2 (1 point)**\n",
        "\n",
        "The next cell will incrementally add a quad-gram to the tree. Plot the perplexity of trigram language model (induced by this count tree) against the number of added n-grams. Comment on the curve shape. Smooth this language model with a zerogram distribution using a linear combination ($0.75\\times p_4 + 0.25\\times p_0$).\n",
        "\n",
        "**1.3 (1 point)**\n",
        "\n",
        "For the given range of thresholds, prune your tree and see how the threshold affects the performance. Plot the results (perplexity vs. threshold).\n",
        "\n",
        "**1.4 (1 point)**\n",
        "\n",
        "1. If you first prune with threshold $k_1$ and get tree $t_1$, then prune with $k_2$ and get $t_2$ what will be the relationship between $t_1$ and $t_2$ if $k_1 \\ge k_2$? (0.25 points)\n",
        "2. What is the memory benefit of count trees, in comparison to storing the counts as a dictionary `{n-gram:freq}`? (0.25 points)\n",
        "3. If we pruned the tree so that only the first level is preserved, what distribution could we model with this tree? (0.25 points)\n",
        "4. Pruning the count tree is said to be a dynamic way of smoothing the language model. Elaborate on how this smoothing happens. (0.25 points)\n",
        "\n",
        "**Answers 1.4**\n",
        "\n",
        "1. For tree $t1$ we will have lesser nodes compared to $t2$. Now n-grams sharing same history(present in the tree) but n-gram not present will be provided a pseudo count which would act as smoothing.\n",
        "\n",
        "2. Count tree is memory effecient. With count trees, same history is present only once in memory with its corresponding count, but for dictionary, same history will be present all the times n-gram occurs.\n",
        "\n",
        "3. When we will have have only one layer, then probability distribution becomes like this : $P(w_1,..w_n / w_1,...w_{n-1}) = C(w_n) / C(w_n - 1)$ where $C$ is the count function. Further we see it is dependent only on the count of the last two tokens of the n-gram. \n",
        "\n",
        "4. Yes, it is a dynamic way of smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_zKzeBzZLksw"
      },
      "outputs": [],
      "source": [
        "from importlib import reload\n",
        "import exercise_1\n",
        "exercise_1 = reload(exercise_1)\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/data/alice_in_wonderland.txt')\n",
        "sys.path.append('/content/exercise_1.py')\n",
        "\n",
        "tree = exercise_1.CountTree(n=4)\n",
        "\n",
        "assert tree.get(\"\") == 0\n",
        "tree.add(\"ABCE\")\n",
        "tree.add(\"ABCD\")\n",
        "tree.add(\"ABCD\")\n",
        "tree.add(\"QBCD\")\n",
        "tree.add(\"QQCD\")\n",
        "tree.add(\"BCDA\")\n",
        "tree.add(\"1234\")\n",
        "tree.add(\"1234\")\n",
        "tree.add(\"1234\")\n",
        "tree.add(\"1234\")\n",
        "tree.add(\"1234\")\n",
        "tree.add(\"5634\")\n",
        "assert tree.get(\"ABCD\") == 2\n",
        "assert tree.get(\"ABCX\") == 0\n",
        "assert tree.get(\"BCD\") == 3\n",
        "assert tree.get(\"D\") == 4\n",
        "assert tree.get(\"CD\") == 4\n",
        "assert tree.get(\"1234\") == 5\n",
        "assert tree.get(\"5634\") == 1\n",
        "tree.prune(4)\n",
        "assert tree.get(\"ABCD\") == 4\n",
        "assert tree.get(\"XXCD\") == 4\n",
        "assert tree.get(\"D\") == 4\n",
        "assert tree.get(\"1234\") == 5\n",
        "assert tree.get(\"5634\") == 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "mZQ7BgD3Xr2j",
        "outputId": "512b72dc-323b-4597-cb7b-65aeafc5a60d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa9bdba70d0>]"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_x = []\n",
        "plot_y = []\n",
        "\n",
        "ngrams = []\n",
        "tree = exercise_1.CountTree(n=4)\n",
        "with open(\"/content/data/alice_in_wonderland.txt\", \"r\") as f:\n",
        "  tokens = f.read().lower().split()\n",
        "  for i in range(len(tokens)-4):\n",
        "    ngrams.append(tokens[i:i+4])\n",
        "\n",
        "vocab = set(tokens)\n",
        "for i,ngram in enumerate(ngrams):\n",
        "  tree.add(ngram)\n",
        "  if i % 1000 == 0:\n",
        "    plot_x.append(i)\n",
        "    plot_y.append(tree.perplexity(ngrams, vocab))\n",
        "\n",
        "plt.plot(plot_x, plot_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "QYPY_Whbooor",
        "outputId": "f0a5fcb5-77f0-409f-ec43-6f70c87b6e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 10, 25, 50, 75, 100]\n",
            "[1.086053479519519, 1.0263567403995828, 1.0119030269366438, 1.0030047011148526, 1.0003161292397378, 0.9976643901733325, 1.0014839387087633, 1.0021067662480476, 1.0047578548709355, 1.0035994553932723]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa9bd8b2710>]"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_x = []\n",
        "plot_y = []\n",
        "\n",
        "for threshold in [1,2,3,4,5,10,25,50,75,100]:\n",
        "  tree.prune(threshold)\n",
        "  plot_x.append(threshold)\n",
        "  plot_y.append(tree.perplexity(ngrams, vocab))\n",
        "\n",
        "print(plot_x)\n",
        "print(plot_y)\n",
        "\n",
        "plt.plot(plot_x, plot_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC_TytVNlgtr"
      },
      "source": [
        "## Exercise 2: Kneser-Ney Smoothing (5 points)\n",
        "\n",
        "This exercise aims to provide a basic understanding of Kneser-Ney Smoothing. Kneser-Ney Smoothing makes use of *continuation counts* of words for lower order n-grams, given as\n",
        "\n",
        "\\begin{equation}\n",
        "C_{KN} = \n",
        "\\begin{cases}\n",
        "\\text{count}(\\bullet) & \\text{for highest order} \\\\\n",
        "\\text{continuationcount}(\\bullet) & \\text{for lower orders}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "For a trigram distribution, Kneser-Ney Smoothing is implemented using the following equations:\n",
        "\n",
        "$$P_{KN}(w_3|w_1, w_2) = \\frac{\\max\\{N(w_1 w_2 w_3)-d,0\\}}{N(w_1 w_2)} + \\lambda(w_1, w_2)P_{KN}(w_3|w_2)$$\n",
        "\n",
        "$$P_{KN}(w_3|w_2) = \\frac{\\max\\{N_{+}(\\bullet w_2 w_3)-d,0\\}}{N_{+}(\\bullet w_2 \\bullet)} + \\lambda(w_2)P_{KN}(w_3)$$\n",
        "\n",
        "\\begin{equation}\n",
        "P_{KN}(w_3) = \\begin{cases}\n",
        "\\frac{N_{+}(\\bullet w_3)}{N_{+}(\\bullet \\bullet)} & \\text{if $w_3 \\in$ V} \\\\\n",
        "\\frac{1}{V} & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "$\\lambda$ is used to normalise the discounted probability mass and is given by\n",
        "\n",
        "$$\\lambda(w_1, w_2) = \\frac{d}{N(w_1 w_2)} \\cdot N_{+}(w_1 w_2 \\bullet)$$\n",
        "\n",
        "$$\\lambda(w_2) = \\frac{d}{N(w_2)} \\cdot N_{+}(w_2 \\bullet)$$\n",
        "\n",
        "**2.1 (4.5 points)**\n",
        "\n",
        "* Your first task is to understand what these terms represent and fill it in the table below (4-5 words each).\n",
        "\n",
        "* Create a trigram-level model on the given text, `alice_in_wonderland.txt`. Write your implementation in the file `exercise_2.py`. Preprocess the text by punctuation removal, lowercasing, and tokenisation. There is no need to split the data into train and test sets. (0.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vNH_zLxM5amO"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/data/alice_in_wonderland.txt')\n",
        "sys.path.append('/content/exercise_2.py')\n",
        "\n",
        "from importlib import reload\n",
        "import exercise_2\n",
        "exercise_2 = reload(exercise_2)\n",
        "\n",
        "file = open(\"/content/data/alice_in_wonderland.txt\", \"r\")\n",
        "text = file.read()\n",
        "\n",
        "# TODO: Preprocess text\n",
        "tokens = exercise_2.preprocess(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw1xFixt2rUa"
      },
      "source": [
        "* \n",
        "Write a simple class `KneserNey` in `exercise_2` that calculates the different parameters required for finding the trigram conditional probability. You may modify the function signature and add other functionality as required. <br/>\n",
        "Now, consider the trigrams `\"alice said nothing\"` and `\"alice said nichts\"`. For these trigrams, estimate the values mentioned in the table given below and fill in the obtained results. The discounting parameter *d* = 0.75. (3 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja0jVjrh2zu8",
        "outputId": "f2b04b85-475b-4653-b2cf-8405aedf4728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Params for t1 \n",
            " N+(∙∙) =  14716\n",
            " N+(∙w3) =  23\n",
            " N+(∙w2∙) =  319\n",
            " N+(∙w2w3) =  5\n",
            " N(w2) =  462\n",
            " N+(w2∙) =  56\n",
            " N(w1w2) =  11\n",
            " N(w1w2w3) =  2\n",
            " N+(w1w2∙) =  11\n",
            " λ(w2) =  0.09090909090909091\n",
            " λ(w1w2) =  0.75\n",
            " \n",
            " Calculating the probabilities:\n",
            " PKN(w3) =  0.0003633720930232558\n",
            " PKN(w3|w2) =  0.013355917839177663\n",
            " PKN(w3|w1,w2) =  0.12365330201574688\n",
            "\n",
            " Params for t2 \n",
            " N+(∙∙) =  14716\n",
            " N+(∙w3) =  0\n",
            " N+(∙w2∙) =  319\n",
            " N+(∙w2w3) =  0\n",
            " N(w2) =  462\n",
            " N+(w2∙) =  56\n",
            " N(w1w2) =  11\n",
            " N(w1w2w3) =  0\n",
            " N+(w1w2∙) =  11\n",
            " λ(w2) =  0.09090909090909091\n",
            " λ(w1w2) =  0.75\n",
            " \n",
            " Calculating the probabilities:\n",
            " PKN(w3) =  0.0003633720930232558\n",
            " PKN(w3|w2) =  3.30338266384778e-05\n",
            " PKN(w3|w1,w2) =  2.4775369978858354e-05\n"
          ]
        }
      ],
      "source": [
        "KN_model = exercise_2.KneserNey(tokens, d=0.75, N=3)\n",
        "\n",
        "t1 = \"alice said nothing\"\n",
        "t2 = \"alice said nichts\"\n",
        "\n",
        "# TODO\n",
        "# Get the required parameters\n",
        "print(\" Params for t1 \")\n",
        "KN_model.get_params(t1)\n",
        "\n",
        "print(\"\\n Params for t2 \")\n",
        "KN_model.get_params(t2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4WFNsNc5YTq"
      },
      "source": [
        "| Term in Kneser-Ney |   Value t1  | Value t2 | Description | \n",
        "|---|---|---| --- |\n",
        "|$N(w_1w_2w_3)$| 2 | 0 | Total frequency of word $(w_1 w_2 w_3)$ |\n",
        "|$N(w_1 w_2)$| 11 | 11 | Total frequency of word $(w_1 w_2)$ |\n",
        "|$N_{+}( \\bullet w_2 w_3)$| 5 | 0 | Number of types of words preceding $w_2 w_3$ |\n",
        "|$N_{+}( \\bullet w_2 \\bullet)$| 319 | 319 | Number of types of words preceding  and following $w_2$ |\n",
        "|$N_{+}( \\bullet w_3)$| 23 | 0 | Number of types of words preceding $w_3$  |\n",
        "|$N_{+}( \\bullet \\bullet)$| 14716 | 14716 | Number of types of bigram words |\n",
        "|$N_{+}(w_1 w_2 \\bullet)$| 11 | 11 | Number of types of words following $w_1 w_2$ |\n",
        "|$N_{+}(w_2 \\bullet)$| 56 | 56 | Number of types of words following $w_2$ |\n",
        "|$\\lambda(w_1 w_2)$| 0.75 | 0.75 | Normalized discount paramter |\n",
        "|$\\lambda(w_2)$| 0.09090909090909091 | 0.09090909090909091 | Normalized discount paramter |\n",
        "\n",
        "* Using the values obtained above, manually calculate $P_{KN}(w_3)$, $P_{KN}(w_3|w_2)$, and $P_{KN}(w_3|w_1, w_2)$ for the given trigrams. (1 point)\n",
        "\n",
        "**Answer**\n",
        "\n",
        "*Probabilities for t1:*\n",
        "\n",
        "$PKN(w3) =  0.0003633720930232558$ \\\n",
        "$PKN(w3|w2) =  0.013355917839177663$ \\\n",
        "$PKN(w3|w1,w2) =  0.12365330201574688$ \\\n",
        "\n",
        "*Probabilities for t2:*\n",
        "\n",
        "$PKN(w3) =  0.0003633720930232558$ \\\n",
        "$PKN(w3|w2) =  3.30338266384778e-05$ \\\n",
        "$PKN(w3|w1,w2) =  2.4775369978858354e-05$ \\\n",
        "\n",
        "\n",
        "**2.2 (0.5 points)**\n",
        "\n",
        "Take a look at this [video](https://www.youtube.com/watch?v=cbAxvpBFyNU) on Kneser-Ney smoothing by Dan Jurafksy. Make sure to undestand his *San Francisco* example. <br/>\n",
        "How will Kneser-Ney Smoothing handle the following bigrams (answer in 3-4 sentences)? \n",
        "\n",
        "* Abu Dhabi\n",
        "\n",
        "* Game Over\n",
        "\n",
        "\n",
        "**Answer 2.2**\n",
        "\n",
        "Kneser-Ney Smoothing is better estimate for probabilties of low order n-grams. Given bigrams \"Abu Dhabi\" and \"Game Over\" can appear several times in the training corpus and hence the frequency of unigrams \"Dhabi\" and \"Over\" will also be high. Relying on only the frequency of \n",
        "unigram to predict the frequency of n-gram can give distorted results. Kneser-Ney smoothing rectify this by considering the frequency of bigrams completed by the given unigram or the number of types of word preceding the unigram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq7Yc1CmKdLi"
      },
      "source": [
        "## Bonus (2 points)\n",
        "\n",
        "For each of the smoothing techniques below,\n",
        "\n",
        "1. Laplace/add-1 smoothing (0.3 points)\n",
        "2. Add-$\\alpha$ smoothing (0.3 points)\n",
        "3. Linear interpolation (0.3 points)\n",
        "4. Absolute discounting (0.3 points)\n",
        "5. Good-Turing (0.3 points)\n",
        "6. Kneser-Ney smoothing (0.3 point)\n",
        "\n",
        "* Give the intuition behind it\n",
        "* State at least one drawback and\n",
        "* Explain how the ensuing smoothing technique accounts for this drawback.\n",
        "\n",
        "You can do so in continuous text or in bullet points. Write 3-5 sentences for each technique. For Kneser-Ney smoothing, you should suggest *and explain* an improved version from the literature, e.g. [here](http://nrs.harvard.edu/urn-3:HUL.InstRepos:25104739) (this tutorial may also be helpful for the rest of the exercise).\n",
        "\n",
        "Please note that while the points for this bonus exercise are the immediate motivation, your self-made comparison will be highly beneficial for the exam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KIBJ8xslBlF"
      },
      "source": [
        "## Bonus : Answers\n",
        "\n",
        "For each of the smoothing techniques below,\n",
        "\n",
        "1. Laplace/add-1 smoothing\n",
        "\n",
        "- Intuition - To ensure our posterior probabilities are never zero, for which we assume every seen or unseen token occurs once more than it did in the training set.\n",
        "- $ P(w_i) = \\_frac{C(w_i + 1)}{N + V} $ where $N$ is number of Tokens in Training Set and $V$ is the Vocabulary Size\n",
        "- Drawback: Using $\\alpha = 1$, we are assigning too much Probability mass to the Unseen tokens\n",
        "- Enuring Smoothing Technique causes the LM to assume the frequency on the unseen words to be more than expected, hence higher probability to assign these unseen token as the next probable word. $w_1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsIFcpt5lmUc"
      },
      "source": [
        "2. Add-$\\alpha$ smoothing \n",
        "    - Intuition: To ensure our posterior probabilities are never zero, for which we assume every seen or unseen token occurs slightly more time than zero. Generally value of $\\alpha$ is in the range $0 < \\alpha < 1$. This is done to avoid assigning too much Probability mass to unseen tokens.\n",
        "    - $P(w_i) = \\frac{C(w_i + \\alpha)}{N + V*\\alpha}$\n",
        "    - Drawback: Using $0 <\\alpha < 1$, we are assigning more Probability to the unseen tokens as compared to the original\n",
        "    - Enuring Smoothing Technique causes the LM to assume the frequency on the unseen words to be more than expected, hence higher probability to assign these unseen token as the next probable word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4lpRuXTlp4w"
      },
      "source": [
        "3. Good-Turing\n",
        "    - Intuition: Redistributing the probability mass of $n$-grams that occur $r + 1$ times in the training data to the $n$-grams that occur $r$ times.\n",
        "    - Adjust Counting    \n",
        "    \\begin{equation}\n",
        "    N^*(w,h) = \\begin{cases}\n",
        "    [N(w,h) + 1]\\frac{n_{N(w,h)+1}}{n_{N(w,h)}} & \\text{for $N(w,h) <= 5$} \\\\\n",
        "    N(w,h) & \\text{otherwise}\n",
        "    \\end{cases}\n",
        "    \\end{equation}\n",
        "    - $P(w|h) = \\frac{N^*(w,h)}{N(h)} + \\lambda(h)\\beta(w|h)$\n",
        "    - Drawback: If $n_{r+1} = 0$, then it causes holes in the count.\n",
        "    - Enuring Smoothing Technique causes amplification of this noise in our training corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrIxhJ_Bls0B"
      },
      "source": [
        "4. Linear interpolation\n",
        "    - Intuition: To solve the sparseness in the above models is by mixing the trigram model with the bigram and unigram models\n",
        "    - $P_li(w_n|w_{n-2},w_{n-1}) = \\lambda_1 P_1(w_n) + \\lambda_2 P_2(w_n|w_{n-1}) +\\lambda_3 P_3(w_n|w_{n-1},w_{n-2})$\n",
        "    \n",
        "    where $0 <= \\lambda_i <= 1$ and $\\sum_i \\lambda_i = 1$ \n",
        "    - Drawback: Tuning all the $\\lambda$'s make it very hard to implement this model\n",
        "    - Enuring Smoothing Technique causes assuming the closest proximity value for $\\lambda$ then calculating the precised values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li4c0FB0lvoo"
      },
      "source": [
        "5. Absolute discounting\n",
        "    - Intuition: Here rather than multiplying by higher order probability with $\\lambda$, we subtract $\\delta$ from each non zero count\n",
        "    \\begin{equation}\n",
        "    P_{abs}(w_1,...,w_n) = \\begin{cases}\n",
        "    \\frac{r-\\delta}{N} & \\text{if $r > 0$} \\\\\n",
        "    \\frac{(B-N_0)\\delta}{N_0N} & \\text{otherwise}\n",
        "    \\end{cases}\n",
        "    \\end{equation}\n",
        "    where B is the number of bins\n",
        "    - Drawback: it does not take into account for those tokens which are mostly seen as bigram, such as \"New York\"\n",
        "    - Enuring Smoothing Technique causes we will have to feed such tokens in the Corpus as one word, else this exception is not taken into account during approximation, we will have to take lower order backing off into account during approximation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBzGLEGlyW4"
      },
      "source": [
        "6. Kneser-Ney smoothing\n",
        "    - Intuition: Here we consider the words with their context an extention to absolute discounting and find the probability distribution as per the common history of the word. For Example \"San Francisco\", using Absolute Discounting “Francisco” will get a higher unigram probability irrespective of the history, but using Kneser-Ney smoothing will give a lower unigram Probability to “Francisco”, as it is more common that it occurs after “San”.\n",
        "    - Modified Kneser-Ney Smoothing: Here rather than using a single discount for all non-zero Counts, this method uses 3 different Discount Parameters $D_1, D_2 and D_3+$, which are applied to one, two and three or more counts to n-gram at each level probability.\n",
        "    - $P_{KN}(w_i|w^{i-1}_{i-n+1}) = \\frac{c(w^i_{i-n+1}) - D(c(w^i_{i-n+1}))}{\\sum_{wi}c(w^i_{i-n+1})} \\gamma(w^i_{i-n+1})P_{KN}(w^i|w^{i-1}_{i-n+2})$\n",
        "      \n",
        "    \\begin{equation}\n",
        "    D(c) = \\begin{cases}\n",
        "    0 & \\text{if $c = 0$} \\\\\n",
        "    D_1 & \\text{if $c = 1$} \\\\\n",
        "    D_2 & \\text{if $c = 2$} \\\\\n",
        "    D_3 & \\text{if $c >= 3$} \n",
        "    \\end{cases}\n",
        "    \\end{equation}\n",
        "    - The modified Kneser Ney Smoothing performs better than the standard in bigram and trigram corpus and over all training set size. And it has similar performance for other corpora. The performance of both is very close, but for larger training set Modified version consistently outperforms the standard version."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment 7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

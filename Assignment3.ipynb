{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0Looy74Lor"
      },
      "source": [
        "## Exercise 1: Entropy Intuition (2 points)\n",
        "\n",
        "### 1.1 (0.5 points)\n",
        "\n",
        "Order the following three snippets by entropy (highest to lowest). Justify your answer (view it more intuitively rather than by using a specific character-level language model, though you would probably reach the same conclusion).\n",
        "\n",
        "```\n",
        "1:    A B A A A A B B A A A B A B B B B B A\n",
        "2:    A B A B A B A B A B A B A B A B A B A\n",
        "3:    A B A A A B A B A B A B A B A B A B A\n",
        "```\n",
        "\n",
        "### 1.2 (0.5 point)\n",
        "\n",
        "Words in natural language do not have the maximum entropy given the available alphabet. This creates a redundancy (e.g. the word `maximum` could be uniquely replaced by `mxmm` and everyone would still understand). If the development of natural languages leads to somewhat optimal solutions, why is it beneficial to have such redundancies in communication?\n",
        "\n",
        "If you're uncertain, please refer to this well-written article: [www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html](http://www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html).\n",
        "\n",
        "### 1.3 (1 point)\n",
        "\n",
        "1. Assume you were given a perfect language model that would always assign probability of $1$ to the next word. What would be the cross-entropy on any text? Motivate your answer with formal derivation. (0.5 points)\n",
        "2. How does cross-entropy relate to perplexity? Is there a reason why would one be preferred over the other? (0.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSI0b8FywEdv"
      },
      "source": [
        "## Exercise 1: Answers\n",
        "\n",
        "### 1.1\n",
        "\n",
        "Order the following three snippets by entropy (highest to lowest). Justify your answer (view it more intuitively rather than by using a specific character-level language model, though you would probably reach the same conclusion).\n",
        "\n",
        "```\n",
        "1:    A B A A A A B B A A A B A B B B B B A\n",
        "2:    A B A B A B A B A B A B A B A B A B A\n",
        "3:    A B A A A B A B A B A B A B A B A B A\n",
        "```\n",
        "Highest to Lowest Entropy: $3 , 2 , 1$ (here 1 and 2 both have same entropy)\n",
        "\n",
        "As we can see the Snippet 3 has more occurance of A in the sequence than B, hence we have more information in 3 than 2 or 1,\n",
        "For Snippets 2 and 1, both have equal occurance of A and B in the sequence, hence we can deduce that we have less information in these snippets as compared to 3.\n",
        "\n",
        "|Snippets|P(A)|P(B)|\n",
        "|-|-|-|\n",
        "|1|$\\frac{10}{19}$|$\\frac{9}{19}$|\n",
        "|2|$\\frac{10}{19}$|$\\frac{9}{19}$|\n",
        "|3|$\\frac{11}{19}$|$\\frac{8}{19}$|\n",
        "\n",
        "### 1.2\n",
        "\n",
        "To obtain Partially Optimal Solution for the Natural Language we require some redundancy in the language, As it increases some relavance in the text. As we use multiple abbreviation in our language for one Word, some redundancy will allow the Natural Language to have any of those abbreviation and hence providing relavance in the information.\n",
        "\n",
        "\n",
        "### 1.3\n",
        "\n",
        "1. If the Language Model is perfect that assign Probability of $1$ to next word, Then it means we have complete information about this Language Model. The Entropy of Such Model is $H = 0$, Thus the cross entropy will not give us any information for such model.\n",
        "\n",
        "\n",
        "2. Perplexity and Cross Entropy\n",
        "In language modeling, we consider a system with state distribution which tries to replicate Real World Systems' Sampled Distribution. Hence we use a Modified Perplexity Model with Cross Entropy.\n",
        "\n",
        "$PP = 2^{H(p,q)} = 2^{E_p [\\log_2(q)]} = 2^{-\\sum_{i=1}^n {p(x_i) \\log_2 q(x_i)}} $\n",
        "\n",
        "When LM in Discrete case, we same N samples sentence ${s_1, s_2, s_3, ... , s_N}$ hence we can replace \"n\" with \"N\" also as we know these sentences are Uniformly Distributed $\\sum_{i=1}^N {p(x_i)} = \\frac{1}{N}$. Rewriting the Equation\n",
        "\n",
        "$PP = 2^{-\\frac{1}{N} \\sum_{i=1}^N \\log_2 q(x_i)} $\n",
        "\n",
        "Perplexity by using the cross-entropy, where the cross-entropy indicates the average number of bits needed to encode one word, and perplexity is the number of words that can be encoded with those bits. Perplexity is preferred, because of the exponent provide more substantial improvement than the equivalent improvement in entropy. Also because initially, the complexity of a language model was reported using a simplistic branching factor measurement that is more similar to perplexity than it is to entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VG5StIq4MVW"
      },
      "source": [
        "## Exercise 2: Harry Potter and the Measure of Uncertainty (4 points)\n",
        "\n",
        "#### 2.1 (2.5 points)\n",
        "\n",
        "Harry, Hermione, and Ron are trying to save the Philosopher's Stone. To do this, they have to cross a series of hurdles to reach the room where the stone is kept. Currently, they are trapped in a chamber whose exit is blocked by fire. On a table before them are 7 potions.\n",
        "\n",
        "|P1|P2|P3|P4|P5|P6|P7|\n",
        "|---|---|---|---|---|---|---|\n",
        "\n",
        "Of these, 6 potions are poisons and only one is the antidote that will get them through the exit. Drinking the poison will not kill them, but will weaken them considerably.\n",
        "\n",
        "1. There is no way of knowing which potion is a poison and which an antidote. How many potions must they sample *on an average* to pick the antidote? (1 point)\n",
        "\n",
        "Hermione notices a scroll lying near the potions. The scroll contains an intricate riddle written by Professor Snape that will help them determine which potion is the antidote. With the help of the clues provided, Hermione cleverly deduces that each potion can be the antidote with a certain probability.\n",
        "\n",
        "|P1|P2|P3|P4|P5|P6|P7|\n",
        "|---|---|---|---|---|---|---|\n",
        "|1/16|1/4|1/64|1/2|1/64|1/32|1/8|\n",
        "\n",
        "2. In this situation, how many potions must they now sample *on an average* to pick the antidote correctly? (1 point)\n",
        "3. What is the most efficient sequence of potions they must sample to discover the antidote? Why do you claim that in terms of how uncertain you are about guessing right? (0.5 point)\n",
        "\n",
        "#### 2.2 (1.5 points)\n",
        "\n",
        "1. Extend your logic from 2.1 to a Shannon's Game where you have to correctly guess the next word in a sentence. Assume that a word is any possible permutation and combination of 26 letters of the alphabet, and all the words have a length of at most *n*.\n",
        "How many guesses will one have to make to guess the correct word? (1 point) <br/>\n",
        "(**Hint**: Think of how many words can exist in this scenario)\n",
        "\n",
        "2. Why is the entropy lower in real-world languages? How do language models help to reduce the uncertainty of guessing the correct word? (2-3 sentences) (0.5 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT5j01jdwEdw"
      },
      "source": [
        "## Exercise 2: Answers\n",
        "\n",
        "#### 2.1\n",
        "\n",
        "1. $H = -\\frac{7}{7} \\log {\\frac{1}{7}} = 2.807$\n",
        "\n",
        "As H is high, Thus we have a lot of missing information.\n",
        "They will have to sample 2.8 Portions on average to identify the Antidote.\n",
        "\n",
        "2. $H = -\\frac{1}{16} \\log {\\frac{1}{16}} -\\frac{2}{64} \\log {\\frac{1}{64}} -\\frac{1}{4} \\log {\\frac{1}{4}} -\\frac{1}{2} \\log {\\frac{1}{2}} -\\frac{1}{32} \\log {\\frac{1}{32}} -\\frac{1}{8} \\log {\\frac{1}{8}}= -\\frac{1}{16} * -4 -\\frac{2}{64} * -6 -\\frac{1}{4} * -2 -\\frac{1}{2} * -1 -\\frac{1}{32} * -5 -\\frac{1}{8} * -3 = \\frac{63}{32} = 1.969 $\n",
        "\n",
        "We have a lit bit more information than the previous stage, H is slightly lower now.\n",
        "They will have to sample 1.9 portions on average to identify the Antidote.\n",
        "\n",
        "3. P4 P2 P7 P1 P6 P3 P5, as P4 has the highest Probability of being an Antidote, We have arranged in increasing probability as we can pick the most probable first.\n",
        "\n",
        "#### 2.2\n",
        "\n",
        "1. For the Above scenario, we have 7 portions, here guessing the Correct word would mean Correctly identifying the Antidote.\n",
        "\n",
        "\n",
        "2. Entropy is lower is the real world languages, as we have few grammar rules which helps in removing Redundancy, hence allowing fewer options, Thus we can say we have more information about real world languages. In the language models, we train it with the most likely character if a character is observed, or most likely if one word occurs, this removes the uncertainity from the language, as we have more information of the Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bx1tnPQ4MgW"
      },
      "source": [
        "## Exercise 3: Kullback-Leibler Divergence (4 points)\n",
        "\n",
        "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
        "\\end{equation}\n",
        "\n",
        "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$.\n",
        "Answer the following questions:\n",
        "\n",
        "#### 3.1. (0.5 points)\n",
        "\n",
        "How is $D_{KL}$ related to Cross-Entropy? Derive a mathematical expression that describes the relationship.\n",
        "\n",
        "\n",
        "#### 3.2. (0.5 points)\n",
        "\n",
        "Is minimizing $D_{KL}$ the same thing as minimizing Cross-Entropy?  Support your answer using your answer to 1.\n",
        "\n",
        "<!-- 3.3. Is $D_{KL}$ a distance metric, i. e. does $D_{KL}(P\\|Q) = D_{KL}(Q\\|P)$ hold? Justify you explanation by a proof or by a numerical counterexample. (1 point) -->\n",
        "\n",
        "\n",
        "#### 3.3 (3 points)\n",
        "\n",
        "For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
        "\n",
        "$\\forall x,y,z \\in U:$\n",
        "\n",
        "1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
        "2. $d(x,y) = d(y,x)$\n",
        "3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
        "\n",
        "Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
        "For each of the three points either prove that it holds for $D_{KL}$ or show a counterexample proving why it does not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "born5NBgwEdx"
      },
      "source": [
        "## Exercise 3: Answers\n",
        "\n",
        "#### 3.1.\n",
        "\n",
        "$D_{KL}$ related to Cross-Entropy\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = -\\sum_{x \\in X}P(x) \\log{Q(x)} + \\sum_{x \\in X}P(x) \\log{P(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = E_{P}[-\\log{Q}] - E_{P}[-\\log{P}]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = H(P,Q) - H(P)\n",
        "\\end{equation}\n",
        "\n",
        "$H(P,Q) -> Cross Entropy$\n",
        "\n",
        "$H(P) -> Entropy$\n",
        "\n",
        "\n",
        "#### 3.2. (0.5 points)\n",
        "\n",
        "Yes, in 3.1 we can see that, Minimizing $D_{KL}$ the same thing as minimizing Cross-Entropy.\n",
        "\n",
        "$D_{KL}(P\\|Q) =  H(P,Q) - H(P)$\n",
        "\n",
        "So if we minimize the cross entropy $H(P,Q)$ then $D_{KL}$ will also decrease.\n",
        "\n",
        "\n",
        "#### 3.3 (3 points)\n",
        "\n",
        "For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
        "\n",
        "$\\forall x,y,z \\in U:$\n",
        "\n",
        "1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
        "2. $d(x,y) = d(y,x)$\n",
        "3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
        "\n",
        "#Property 1.\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
        "\\end{equation}\n",
        "\n",
        "If $P = Q$, then\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|P) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{P(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|P) = \\sum_{x \\in X}P(x) \\cdot \\log (1)\n",
        "\\end{equation}\n",
        "\n",
        "As $log (1) = 0$, hence\n",
        "\\begin{equation}\n",
        "D_{KL}(P\\|P) = 0\n",
        "\\end{equation}\n",
        "\n",
        "Hence we can say that property $1$ is true.\n",
        "\n",
        "#Property 2.\n",
        "$X = {0, 1, 2}$\n",
        "\n",
        "$U : X$ ~ $Uni(0,2)$         Uni(a,b) is Uniform discrete distribution\n",
        "\n",
        "$G : X$ ~ $Geo(0.5)$         Geo(p) is Geometric distribution\n",
        "\n",
        "Probabilty mass function of the Uniform discrete distribution :\n",
        "\n",
        "\\begin{equation}\n",
        "u(x) =\n",
        "\\begin{array}{rl}\n",
        "1/3 \\; , & \\text{if} \\; x = 0 \\\\\n",
        "1/3 \\; , & \\text{if} \\; x = 1 \\\\\n",
        "1/3 \\; , & \\text{if} \\; x = 2\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "Probabilty mass function of the Geometric distribution :\n",
        "\n",
        "\\begin{equation}\n",
        "g(x) =\n",
        "\\begin{array}{rl}\n",
        "1/2 \\; , & \\text{if} \\; x = 0 \\\\\n",
        "1/4 \\; , & \\text{if} \\; x = 1 \\\\\n",
        "1/8 \\; , & \\text{if} \\; x = 2\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "We will compute $D_{KL}(U\\|G)$,\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\sum_{x \\in X}u(x) \\cdot \\log \\frac{u(x)}{g(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{2}} + \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{4}} + \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{2}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * [\\log \\frac{1}{3} + \\log \\frac{4}{3} + \\log \\frac{8}{3}]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * \\log (\\frac{1}{3} * \\frac{4}{3} * \\frac{8}{3})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * \\log (\\frac{1}{3} * \\frac{4}{3} * \\frac{8}{3})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * \\log (\\frac{64}{27})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = 0.41\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We will compute $D_{KL}(G\\|U)$,\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(G\\|U) = \\sum_{x \\in X}g(x) \\cdot \\log \\frac{g(x)}{u(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(G\\|U) = \\frac{1}{2} * \\log \\frac{\\frac{1}{2}}{\\frac{1}{3}} + \\frac{1}{4} * \\log \\frac{\\frac{1}{4}}{\\frac{1}{3}} + \\frac{1}{8} * \\log \\frac{\\frac{1}{8}}{\\frac{1}{3}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(G\\|U) = \\frac{1}{2} * \\log \\frac{3}{2} + \\frac{1}{4} * \\log \\frac{3}{4} + \\frac{1}{8} * \\log \\frac{3}{8}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(G\\|U) = \\frac{1}{3} * [\\log \\frac{1}{3} + \\log \\frac{4}{3} + \\log \\frac{8}{3}]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(G\\|U) = 0\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Since $D_{KL}(U\\|G) \\neq D_{KL}(G\\|U)$, we can say that property $2$ is not true.\n",
        "\n",
        "#Property 3.\n",
        "\n",
        "$X = {0, 1, 2}$\n",
        "\n",
        "$U : X$ ~ $Uni(0,2)$         Uni(a,b) is Uniform descrete distribution\n",
        "\n",
        "$B : X$ ~ $Bin(2,0.5)$       Bin(n,p) is Binomial distribution\n",
        "\n",
        "$G : X$ ~ $Geo(0.5)$         Geo(p) is Geometric distribution\n",
        "\n",
        "Probabilty mass function of the Uniform discrete distribution :\n",
        "\n",
        "\\begin{equation}\n",
        "u(x) =\n",
        "\\begin{array}{rl}\n",
        "1/3 \\; , & \\text{if} \\; x = 0 \\\\\n",
        "1/3 \\; , & \\text{if} \\; x = 1 \\\\\n",
        "1/3 \\; , & \\text{if} \\; x = 2\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "Probabilty mass function of the Binomial distribution :\n",
        "\n",
        "\\begin{equation}\n",
        "b(x) =\n",
        "\\begin{array}{rl}\n",
        "1/4 \\; , & \\text{if} \\; x = 0 \\\\\n",
        "1/2 \\; , & \\text{if} \\; x = 1 \\\\\n",
        "1/4 \\; , & \\text{if} \\; x = 2\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "Probabilty mass function of the Geometric distribution :\n",
        "\n",
        "\\begin{equation}\n",
        "g(x) =\n",
        "\\begin{array}{rl}\n",
        "1/2 \\; , & \\text{if} \\; x = 0 \\\\\n",
        "1/4 \\; , & \\text{if} \\; x = 1 \\\\\n",
        "1/8 \\; , & \\text{if} \\; x = 2\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We will check if following condition is true,\n",
        "\\begin{equation}\n",
        "D_{KL}(B\\|G) \\leq D_{KL}(U\\|B) + D_{KL}(U\\|G)\n",
        "\\end{equation}\n",
        "\n",
        "First we will compute $D_{KL}(B\\|G)$,\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(B\\|G) = \\sum_{x \\in X}b(x) \\cdot \\log \\frac{b(x)}{g(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(B\\|G) = \\frac{1}{4} * \\log \\frac{\\frac{1}{4}}{\\frac{1}{2}} + \\frac{1}{2} * \\log \\frac{\\frac{1}{2}}{\\frac{1}{4}} + \\frac{1}{4} * \\log \\frac{\\frac{1}{4}}{\\frac{1}{8}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(B\\|G) = \\frac{1}{4} * \\log \\frac{1}{2} + \\frac{1}{2} * \\log 2 + \\frac{1}{8} * \\log 2\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(B\\|G) = \\frac{1}{4} * \\log \\frac{1}{2} + \\frac{3}{4} * \\log 2\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(B\\|G) = 0.5\n",
        "\\end{equation}\n",
        "\n",
        "Now we will compute $D_{KL}(U\\|B)$,\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|B) = \\sum_{x \\in X}u(x) \\cdot \\log \\frac{u(x)}{b(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|B) = \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{4}} + \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{2}} + \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{4}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|B) = \\frac{1}{3} * \\log \\frac{4}{3} + \\frac{1}{3} * \\log  \\frac{2}{3} + \\frac{1}{3} * \\log \\frac{1}{3}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|B) = \\frac{2}{3} * \\log \\frac{4}{3} + \\frac{1}{3} * \\log \\frac{2}{3}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|B) = 0.08\n",
        "\\end{equation}\n",
        "\n",
        "At last we will compute $D_{KL}(U\\|G)$,\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\sum_{x \\in X}u(x) \\cdot \\log \\frac{u(x)}{g(x)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{2}} + \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{4}} + \\frac{1}{3} * \\log \\frac{\\frac{1}{3}}{\\frac{1}{8}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = \\frac{1}{3} * [\\log \\frac{2}{3} + \\log  \\frac{4}{3} + \\log \\frac{8}{3}]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "D_{KL}(U\\|G) = 0.41\n",
        "\\end{equation}\n",
        "\n",
        "$0.5 \\leq 0.08 + 0.49$\n",
        "\n",
        "Above condition is not true, hence we can say that propertiy $3$ is not true.\n",
        "\n",
        "$D_{KL}$ is not a distance metric because all $3$ properties are not true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8zkUP3l4Mxw"
      },
      "source": [
        "## Bonus (1.5 points)\n",
        "\n",
        "1. Compute $D_{KL}(Q_1\\|P_1)$ for the following pair of sentences based on a unigram language model (word level).\n",
        "\n",
        "```\n",
        "p1: to be or not to be\n",
        "q1: to be or to be or not or to be be be\n",
        "```\n",
        "\n",
        " Do so by implementing the function `dkl` in `bonus.py`. You will also have to calculate the distributions $P_1$, $Q_1$; for this, you can either reuse your code from the last assignment or implement a new function in `bonus.py`. (1 point)\n",
        "\n",
        "2. Suppose the sentences in 1. would be replaced by the following sequences of symbols. You can imagine them to be sequences of nucleobases in a [coding](https://en.wikipedia.org/wiki/Coding_region) region of a gene in your genome.\n",
        "\n",
        "```\n",
        "p2: ACTGACACTGAC\n",
        "q2: ACTACTGACCCACTACTGACCC\n",
        "```\n",
        "\n",
        "Let $P_2$, $Q_2$ be the character-level unigram LMs derived from these sequences. What values will $D_{KL}(P_1\\|P_2)$, $D_{KL}(Q_1\\|Q_2)$ take? Does the quantity hold any information? Would computing $D_{KL}$ between distributions over two different natural languages hold any information? (0.5 points)\n",
        "\n",
        "No mathematical explanation nor coding required for the second part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fBOuBNr6FY8",
        "outputId": "32966415-eb5b-46b2-8bca-a16f21cb4223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09328462323031254\n"
          ]
        }
      ],
      "source": [
        "from importlib import reload\n",
        "import bonus\n",
        "bonus = reload(bonus)\n",
        "\n",
        "# TODO: estimate LMs\n",
        "P = \"to be or not to be\"\n",
        "Q = \"to be or to be or not or to be be be\"\n",
        "\n",
        "# TODO: DKL The answer is in Bits for which we used Log base 2\n",
        "print(bonus.dkl(P,Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNLMPhNRwEdz"
      },
      "source": [
        "## Bonus : Answers\n",
        "\n",
        "2.\n",
        "Values for $D_{KL}(P_1\\|P_2)$, $D_{KL}(Q_1\\|Q_2)$ will be greather than 0. Yes, quantity will hold information about the difference between two language models. Yes computing $D_{KL}$ between distributions over two different natural language holds information about the degree of difference between them."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
